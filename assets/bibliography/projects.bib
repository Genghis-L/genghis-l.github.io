@article{bartlett_convexity_2006,
	title = {Convexity, Classification, and Risk Bounds},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000000907},
	doi = {10.1198/016214505000000907},
	abstract = {Many of the classiﬁcation algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally eﬃcient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function: that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled convex hulls of a ﬁnite-dimensional base class, with a variety of commonly used loss functions.},
	pages = {138--156},
	number = {473},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Bartlett, Peter L and Jordan, Michael I and {McAuliffe}, Jon D},
	urldate = {2024-12-17},
	date = {2006-03},
	langid = {english},
}

@article{mao_cross-entropy_2023,
	title = {Cross-Entropy Loss Functions: Theoretical Analysis and Applications},
	abstract = {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the ﬁrst H-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the speciﬁc hypothesis set H used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a speciﬁc analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneﬁcial in the adversarial setting by proving that they admit Hconsistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.},
	journaltitle = {Proceedings of the 40th International Conference on Machine Learning},
	author = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
	date = {2023},
	langid = {english},
}

@article{awasthi_theoretically_2023,
	title = {Theoretically Grounded Loss Functions and Algorithms for Adversarial Robustness},
	abstract = {Adversarial robustness is a critical property of classiﬁers in applications as they are increasingly deployed in complex real-world systems. Yet, achieving accurate adversarial robustness in machine learning remains a persistent challenge and the choice of the surrogate loss function used for training a key factor. We present a family of new loss functions for adversarial robustness, smooth adversarial losses, which we show can be derived in a general way from broad families of loss functions used in multi-class classiﬁcation. We prove strong H-consistency theoretical guarantees for these loss functions, including multi-class H-consistency bounds for sum losses in the adversarial setting. We design new regularized algorithms based on the minimization of these principled smooth adversarial losses ({PSAL}). We further show through a series of extensive experiments with the {CIFAR}-10, {CIFAR}100 and {SVHN} datasets that our {PSAL} algorithm consistently outperforms the current stateof-the-art technique, {TRADES}, for both robust accuracy against ∞-norm bounded perturbations and, even more signiﬁcantly, for clean accuracy. Finally, we prove that, unlike {PSAL}, the {TRADES} loss in general does not admit an H-consistency property.},
	journaltitle = {Proceedings of the 26th International Conference on Artificial Intelligence and Statistics},
	author = {Awasthi, Pranjal and Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
	date = {2023},
	langid = {english},
}

@article{zhang_theoretically_2019,
	title = {Theoretically Principled Trade-off between Robustness and Accuracy},
	abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classiﬁcation) error and boundary error, and provide a differentiable upper bound using the theory of classiﬁcation-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, {TRADES}, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the {NeurIPS} 2018 Adversarial Vision Challenge in which we won the 1st place out of {\textasciitilde}2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean `2 perturbation distance.},
	journaltitle = {Proceedings of the 36th International Conference on Machine Learning},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P and Ghaoui, Laurent El and Jordan, Michael I},
	date = {2019},
	langid = {english},
}

@article{ma_towards_2018,
	title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classiﬁed incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-deﬁned class of adversaries. These methods let us train networks with signiﬁcantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a ﬁrst-order adversary as a natural security guarantee. We believe that robustness against such well-deﬁned classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	journaltitle = {International Conference on Learning Representations},
	author = {Ma, Aleksander},
	date = {2018},
	langid = {english},
}

@misc{pires_multiclass_2016,
	title = {Multiclass Classification Calibration Functions},
	url = {http://arxiv.org/abs/1609.06385},
	doi = {10.48550/arXiv.1609.06385},
	abstract = {In this paper we reﬁne the process of computing calibration functions for a number of multiclass classiﬁcation surrogate losses. Calibration functions are a powerful tool for easily converting bounds for the surrogate risk (which can be computed through well-known methods) into bounds for the true risk, the probability of making a mistake. They are particularly suitable in non-parametric settings, where the approximation error can be controlled, and provide tighter bounds than the common technique of upper-bounding the 0-1 loss by the surrogate loss.},
	number = {{arXiv}:1609.06385},
	publisher = {{arXiv}},
	author = {Pires, Bernardo Ávila and Szepesvári, Csaba},
	urldate = {2024-12-17},
	date = {2016-09-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1609.06385},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{carlini_adversarial_2017,
	title = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
	url = {http://arxiv.org/abs/1705.07263},
	doi = {10.48550/arXiv.1705.07263},
	shorttitle = {Adversarial Examples Are Not Easily Detected},
	abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
	number = {{arXiv}:1705.07263},
	publisher = {{arXiv}},
	author = {Carlini, Nicholas and Wagner, David},
	urldate = {2024-12-17},
	date = {2017-11-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1705.07263 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@misc{papernot_distillation_2016,
	title = {Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks},
	url = {http://arxiv.org/abs/1511.04508},
	doi = {10.48550/arXiv.1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network ({DNN}) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the {DNN}, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on {DNNs}. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training {DNNs}. We also empirically study the effectiveness of our defense mechanisms on two {DNNs} placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied {DNN}. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the {DNNs} we tested.},
	number = {{arXiv}:1511.04508},
	publisher = {{arXiv}},
	author = {Papernot, Nicolas and {McDaniel}, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	urldate = {2024-12-17},
	date = {2016-03-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.04508 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and Harnessing Adversarial Examples},
	url = {http://arxiv.org/abs/1412.6572},
	doi = {10.48550/arXiv.1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the {MNIST} dataset.},
	number = {{arXiv}:1412.6572},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	urldate = {2024-12-17},
	date = {2015-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1412.6572 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}
